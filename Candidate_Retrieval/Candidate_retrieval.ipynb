{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9ecca2",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b89ae98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\anaconda3\\envs\\IR_final\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from ast import literal_eval as string_to_list\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, normalize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f1180",
   "metadata": {},
   "source": [
    "## Read in dataset + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "711a7ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv\n",
    "games = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), 'backloggd_games.csv'), index_col=0)\n",
    "\n",
    "# Clean data types + fill missing values + drop duplicates\n",
    "games['Summary'] = games['Summary'].fillna('')\n",
    "games[['Plays','Playing','Backlogs','Wishlist','Lists','Reviews']] = games[['Plays','Playing','Backlogs','Wishlist','Lists','Reviews']]     \\\n",
    "                                                                        .map(lambda x: float(x.replace('K','')) * 1000 if 'K' in x else float(x))\n",
    "games[['Developers','Platforms','Genres']] = games[['Developers','Platforms','Genres']].map(string_to_list)\n",
    "games = games.drop_duplicates(subset='Title', ignore_index=True)\n",
    "\n",
    "# Creates dataset copies for BM-25 and SBERT\n",
    "games_BM25 = games.copy()\n",
    "games_BM25[['Developers','Platforms','Genres']] = games_BM25[['Developers','Platforms','Genres']].map(lambda listed: [x.lower() for x in listed])\n",
    "games_BM25['Title'] = games_BM25['Title'].str.lower()\n",
    "games_BM25['Summary'] = games_BM25['Summary'].str.lower()\n",
    "games_BM25[['Developers','Platforms','Genres']] = games_BM25[['Developers','Platforms','Genres']].map(lambda x: ' '.join(x))\n",
    "\n",
    "games_SBERT = games.copy()\n",
    "games_SBERT[['Developers','Platforms','Genres']] = games_SBERT[['Developers','Platforms','Genres']].map(lambda listed: [x.lower() for x in listed])\n",
    "games_SBERT['Title'] = games_SBERT['Title'].str.lower()\n",
    "games_SBERT['Summary'] = games_SBERT['Summary'].str.lower()\n",
    "\n",
    "# Reads in processed queries and confirms list data types\n",
    "queries = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), 'Query_processing', 'processed_queries.csv'))\n",
    "queries[['Developers','Platforms','Genres']] = queries[['Developers','Platforms','Genres']].map(string_to_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717b848",
   "metadata": {},
   "source": [
    "## BM-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7122128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25_field_matrix(df, game_attribute, k_1=1.2, b=0.8, max_features=50000, min_df=2):\n",
    "    \"\"\"\n",
    "    Creates a BM-25 matrix for an individual attribute in the dataset\n",
    "    Hyperparameters can be tuned to improve results\n",
    "    Code is identical to the pre-processing implementation\n",
    "    \"\"\"\n",
    "\n",
    "    documents = df[game_attribute].to_list()\n",
    "    pipe = Pipeline([('count', CountVectorizer(max_features=max_features, min_df=min_df)), ('tfid', TfidfTransformer())]).fit(documents)\n",
    "    term_doc_matrix = pipe['count'].transform(documents)\n",
    "    doc_lengths, avg_dl, idfs, tfs = term_doc_matrix.sum(axis=1), np.mean(term_doc_matrix.sum(axis=1)), pipe['tfid'].idf_.reshape(1, -1), term_doc_matrix.multiply(1 / term_doc_matrix.sum(axis=1))\n",
    "    numerator = (k_1 + 1) * tfs\n",
    "    denominator = k_1 * ((1 - b) + b * (doc_lengths / avg_dl)) + tfs\n",
    "    BM25 = numerator.multiply(1 / denominator)\n",
    "    BM25 = BM25.multiply(idfs)\n",
    "    vocab = pipe['count'].get_feature_names_out()\n",
    "    vocab = {term:index for index, term in enumerate(vocab)}\n",
    "\n",
    "    return BM25.tocsr(), vocab\n",
    "\n",
    "def retrieve_top_k_bm25(query, bm25_list, weights, doc_titles, top_k=100, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Filters each of the individual BM-25 matrices based on a single query then weights and sums them up together\n",
    "    Epsilon is again used for Out-Of-Vocabulary (OOV) terms\n",
    "    Returns the top k results from the combined weighted BM-25 matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def bm25_filtered(query, bm25):\n",
    "        \"\"\"\n",
    "        Filters an individual BM-25 matrix according to the input query\n",
    "        Code is identical to the BM-25 result generating in the pre-processing\n",
    "        \"\"\"\n",
    "\n",
    "        matrix, vocab = bm25\n",
    "        query = query.split(' ')\n",
    "        query_tokens = [vocab[term] if term in vocab else 'OOV' for term in query]\n",
    "        IV = [term for term in query_tokens if term != 'OOV']\n",
    "        if len(IV) != 0:\n",
    "            doc_scores = matrix[:, IV].sum(axis=1)\n",
    "            if 'OOV' in query:\n",
    "                doc_scores += np.full((matrix.shape[0], 1), epsilon)\n",
    "        else:\n",
    "            doc_scores = np.full((matrix.shape[0], 1), epsilon)\n",
    "\n",
    "        return doc_scores\n",
    "\n",
    "    # Filters and weights the BM-25 matrices\n",
    "    bm25_weighted = np.zeros((bm25_list[0][0].shape[0], 1))\n",
    "    for bm25, weight in zip(bm25_list, weights):\n",
    "        bm25_weighted += weight * bm25_filtered(query, bm25)\n",
    "\n",
    "    # Sorts the results according to the BM-25 score and filters to only the top k -> ((doc_id, doc_title), score)\n",
    "    scores = np.ravel(bm25_weighted)\n",
    "    ranked = sorted(zip(enumerate(doc_titles), scores), key=lambda zipper: zipper[1], reverse=True)\n",
    "    ranked_topk = ranked[:top_k]\n",
    "\n",
    "    # Reformats results -> (query, doc_title, doc_id, score)\n",
    "    output = []\n",
    "    for k in ranked_topk:\n",
    "        doc_id, doc_title, score = k[0][0], k[0][1], k[1]\n",
    "        output.append((query, doc_title, doc_id, score, 0))    \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bca734",
   "metadata": {},
   "source": [
    "### Generate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8fe3096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_22896\\3639212086.py:11: RuntimeWarning: divide by zero encountered in divide\n",
      "  doc_lengths, avg_dl, idfs, tfs = term_doc_matrix.sum(axis=1), np.mean(term_doc_matrix.sum(axis=1)), pipe['tfid'].idf_.reshape(1, -1), term_doc_matrix.multiply(1 / term_doc_matrix.sum(axis=1))\n"
     ]
    }
   ],
   "source": [
    "bm25_matrices = [\n",
    "    BM25_field_matrix(games_BM25, 'Title', k_1=1.2, b=0.4, max_features=5000, min_df=1),\n",
    "    BM25_field_matrix(games_BM25, 'Developers', k_1=1.1, b=0.3, max_features=4000, min_df=1),\n",
    "    BM25_field_matrix(games_BM25, 'Summary', k_1=1.8, b=0.8, max_features=20000, min_df=2),\n",
    "    BM25_field_matrix(games_BM25, 'Platforms', k_1=1.0, b=0.2, max_features=500, min_df=2),\n",
    "    BM25_field_matrix(games_BM25, 'Genres', k_1=1.0, b=0.2, max_features=800, min_df=2)\n",
    "        ]\n",
    "weights = [2.0, 0.6, 1.5, 0.8, 0.8]\n",
    "\n",
    "results_bm25 = []\n",
    "for query in queries['Processed']:\n",
    "    topk = retrieve_top_k_bm25(query, bm25_matrices, weights, games['Title'])\n",
    "    for k in topk: results_bm25.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966929c2",
   "metadata": {},
   "source": [
    "## SBERT + FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e0a7726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1281/1281 [00:30<00:00, 42.07it/s] \n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')     # Initialise SBERT\n",
    "\n",
    "# Embed the text attributes of the dataset, normalisation will occur after combining with the one-hot embeddings\n",
    "embeddings = model.encode(games_SBERT['Title'] +  ' ' + games_SBERT['Summary'], show_progress_bar=True, normalize_embeddings=False)\n",
    "\n",
    "# Fit and transform separate binarisers for the list attributes -> Same binarisers are used to convert the list attributes of the queries\n",
    "mlb1, mlb2, mlb3 = MultiLabelBinarizer(), MultiLabelBinarizer(), MultiLabelBinarizer()\n",
    "developer_onehot, platform_onehot, genre_onehot = mlb1.fit_transform(games_SBERT['Developers']), mlb2.fit_transform(games_SBERT['Platforms']), mlb3.fit_transform(games_SBERT['Genres'])\n",
    "\n",
    "# Concatenates the embeddings and weights each one\n",
    "# Result is normalised to ready the embeddings for cosine similarity\n",
    "weights = [0.5, 0.2, 0.3, 0.4]\n",
    "doc_embeddings = normalize(np.hstack((\n",
    "                    weights[0] * embeddings,\n",
    "                    weights[1] * developer_onehot,\n",
    "                    weights[2] * platform_onehot,\n",
    "                    weights[3] * genre_onehot\n",
    "                    )), norm='l2', axis=1)\n",
    "\n",
    "# Creates the FAISS index according to the generated embeddings\n",
    "data_index = faiss.IndexFlatIP(doc_embeddings.shape[1])\n",
    "data_index.add(doc_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cadaeb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 48.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encodes the text component of the processed queries\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(queries['Processed'], show_progress_bar=True, normalize_embeddings=False)\n",
    "\n",
    "# One-hot encodes the list attributes of the queries\n",
    "developer_onehot, platform_onehot, genre_onehot = mlb1.transform(queries['Developers']), mlb2.transform(queries['Platforms']), mlb3.transform(queries['Genres'])\n",
    "\n",
    "# Combines the query embeddings using the same weights \n",
    "query_embeddings = normalize(np.hstack((\n",
    "                    embeddings,\n",
    "                    developer_onehot,\n",
    "                    platform_onehot,\n",
    "                    genre_onehot\n",
    "                    )), norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45b054",
   "metadata": {},
   "source": [
    "### Generate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf8211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k_faiss(data_index, queries, query_embeddings, doc_titles, top_k=100):\n",
    "    \"\"\"\n",
    "    Searches the FAISS index according to the query embedding matrix\n",
    "    Returns 100 document IDs per query\n",
    "    \"\"\"\n",
    "\n",
    "    # Returns the distances and indexes for each of the returned documents\n",
    "    distance, index = data_index.search(query_embeddings, top_k)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    # Loops through each of the queries\n",
    "    for i, query in zip(range(index.shape[0]), queries):\n",
    "\n",
    "        # Loops through each of the documents in the query results and appends the results in the same format as the BM-25 results -> (query, doc_title, doc_id, score)\n",
    "        for id, score in zip(index[i], distance[i]):\n",
    "            results.append((query, doc_titles[id], id, 0, score))\n",
    "\n",
    "    return results\n",
    "\n",
    "results_faiss = retrieve_top_k_faiss(data_index, queries['Processed'], query_embeddings, games['Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45499e21",
   "metadata": {},
   "source": [
    "## Result Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f899b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bm25_df = pd.DataFrame(results_bm25, columns=['Query', 'Title', 'ID', 'BM25 Score', 'SBERT Score'])\n",
    "results_faiss_df = pd.DataFrame(results_faiss, columns=['Query', 'Title', 'ID', 'BM25 Score', 'SBERT Score'])\n",
    "results_pooled = pd.concat([results_bm25_df, results_faiss_df], axis=0).drop_duplicates(subset=['Query','ID']).sort_values(by='Query', ignore_index=True)\n",
    "results_pooled = results_pooled.rename(columns={'Query':'Processed Query'})\n",
    "results_pooled = pd.merge(results_pooled, queries[['Original','Processed']], how='left', left_on='Processed Query', right_on='Processed')\n",
    "results_pooled = results_pooled.drop(columns=['Processed']).reindex(columns=['Original', 'Processed Query', 'Title', 'ID', 'BM25 Score', 'SBERT Score'])\n",
    "results_pooled = results_pooled.rename(columns={'Original':'Original Query'})\n",
    "\n",
    "results_pooled.to_csv('pooled_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a516dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
