{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b02756cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sean/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/sean/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sean/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "from ast import literal_eval as string_to_list\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "queries = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), 'queries.csv'))\n",
    "games = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), 'backloggd_games.csv'), index_col=0)\n",
    "games['Summary'] = games['Summary'].fillna('')\n",
    "games = games.drop_duplicates(subset='Title', ignore_index=True)\n",
    "games[['Plays','Playing','Backlogs','Wishlist','Lists','Reviews']] = games[['Plays','Playing','Backlogs','Wishlist','Lists','Reviews']]     \\\n",
    "                                                                        .map(lambda x: float(x.replace('K','')) * 1000 if 'K' in x else float(x) * 1000)\n",
    "games[['Developers','Platforms','Genres']] = games[['Developers','Platforms','Genres']].map(string_to_list)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_processing(query, expansion_terms, developers, platforms, genres):\n",
    "    def query_normalisation(query):\n",
    "        # 1. Lowercase\n",
    "        query = query.lower()\n",
    "        \n",
    "        # 2. Remove accents\n",
    "        query = unicodedata.normalize('NFKD', query)\n",
    "        query = ''.join([c for c in query if not unicodedata.combining(c)])\n",
    "        # query = unicodedata.normalize('NFKD', query).encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "        # 3. Remove punctuation\n",
    "        query = query.replace('-', ' ')\n",
    "        query = re.sub(f'[{re.escape(string.punctuation)}]', '', query)\n",
    "        query = re.sub(r'\\s+', ' ', query).strip()\n",
    "        \n",
    "        # 4. Remove stopwords and lemmatise\n",
    "        tokens = word_tokenize(query)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        lemmatiser = WordNetLemmatizer()\n",
    "        lemmatised = [lemmatiser.lemmatize(word) for word in tokens]\n",
    "        return ' '.join(lemmatised)\n",
    "\n",
    "    def query_expansion(query, expansion_terms, synonym_expansion=True):\n",
    "        if synonym_expansion is True:\n",
    "            synonym_expansion = 2\n",
    "        def get_synonyms(token):\n",
    "            synonyms = set()\n",
    "            for syn in wordnet.synsets(token):\n",
    "                for lemma in syn.lemmas():\n",
    "                    # WordNet uses underscores for multi-word synonyms\n",
    "                    # e.g. air_conditioner -> air conditioner\n",
    "                    synonym = lemma.name().replace('_', ' ')\n",
    "                    if synonym.lower() != token.lower():\n",
    "                        synonyms.add(synonym)\n",
    "            return list(synonyms)\n",
    "\n",
    "        # Expand the query with synonyms in domain-specific terms\n",
    "        for term in expansion_terms:\n",
    "            if term in query:\n",
    "                loc = query.find(term)\n",
    "                query = query[:loc] + expansion_terms[term] + ' ' + query[loc:]\n",
    "\n",
    "        expanded_tokens = []\n",
    "        for token in query.split(' '):\n",
    "            expanded_tokens.append(token)\n",
    "            if synonym_expansion:\n",
    "                synonyms = get_synonyms(token)\n",
    "                expanded_tokens.extend(synonyms[:synonym_expansion])\n",
    "\n",
    "        # TODO: Do we necessarily want to remove duplicates?\n",
    "        unique_terms = []\n",
    "        for token in expanded_tokens:\n",
    "            if token not in unique_terms:\n",
    "                unique_terms.append(token)\n",
    "            \n",
    "        return ' '.join(unique_terms)\n",
    "\n",
    "    def query_parsing(query, developers, platforms, genres):\n",
    "        '''\n",
    "        Parse the query to extract developers, platforms, genres, and years.\n",
    "        '''\n",
    "        def extract_years(query):\n",
    "            # Regex pattern: matches 1980–1989, 1990–1999, 2000–2099\n",
    "            pattern = r'\\b(19[8-9]\\d|20\\d{2})\\b'\n",
    "            matches = re.findall(pattern, query)\n",
    "            return [int(year) for year in matches]\n",
    "        \n",
    "        query_years = extract_years(query)\n",
    "        query_developers = [developer for developer in developers if developer in query.split(' ')]\n",
    "        query_platforms = [platform for platform in platforms if platform in query.split(' ')]\n",
    "        query_genres = [genre for genre in genres if genre in query.split(' ')]\n",
    "        return {'Developers':query_developers, 'Platforms':query_platforms, 'Genres':query_genres, 'Years':query_years}\n",
    "\n",
    "    normalised_expanded = query_expansion(query_normalisation(query), expansion_terms, synonym_expansion=False)\n",
    "    parsed = query_parsing(normalised_expanded, developers, platforms, genres)\n",
    "    output = {'Original':query, 'Processed':normalised_expanded, 'Developers':parsed['Developers'], 'Platforms':parsed['Platforms'], 'Genres':parsed['Genres']}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2fdad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_set, platform_set, genre_set = set(), set(), set()\n",
    "for developers, platforms, genres in zip(games['Developers'], games['Platforms'], games['Genres']):\n",
    "    developer_set.update(set(developers)), platform_set.update(set(platforms)), genre_set.update(set(genres))\n",
    "developer_set, platform_set, genre_set = set(word.lower() for word in developer_set), set(word.lower() for word in platform_set), set(word.lower() for word in genre_set)\n",
    "\n",
    "with open('expansion_terms.json', 'r') as json_file:\n",
    "    expansion_terms = json.load(json_file)\n",
    "\n",
    "processed_queries = [query_processing(query, expansion_terms, developer_set, platform_set, genre_set) for query in queries['Query']]\n",
    "processed_queries = pd.DataFrame(processed_queries)\n",
    "processed_queries.to_csv('processed_queries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfa9bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import pandas as pd\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords, wordnet\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from spellchecker import SpellChecker\n",
    "# import spacy\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# import subprocess\n",
    "# import sys\n",
    "# try:\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "# except OSError:\n",
    "#     subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# spell = SpellChecker()\n",
    "# def get_synonyms(word):\n",
    "#     synonyms = set()\n",
    "#     for syn in wordnet.synsets(word):\n",
    "#         for lemma in syn.lemmas():\n",
    "#             synonyms.add(lemma.name().replace('_', ' '))\n",
    "#     return list(synonyms)\n",
    "# def process_query(query):\n",
    "#     query = query.lower()\n",
    "#     query = re.sub(r\"[^\\w\\s-]\", \"\", query)\n",
    "#     query = re.sub(r\"[-]\", \" \", query)\n",
    "#     token = word_tokenize(query)\n",
    "#     tokens = [\n",
    "#         spell.correction(word)\n",
    "#         for word in token\n",
    "#         if word not in stop_words and spell.correction(word) is not None\n",
    "#     ]\n",
    "#     doc = nlp(' '.join(tokens))\n",
    "#     important_terms = [token.lemma_ for token in doc if token.pos_ in ['NOUN', 'PROPN', 'VERB']]\n",
    "#     expanded = set(important_terms)\n",
    "#     for word in important_terms:\n",
    "#         synonyms = get_synonyms(word)\n",
    "#         expanded.update(synonyms[:2])\n",
    "#     return list(expanded)\n",
    "# df = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), 'queries.csv'))\n",
    "# df['processed'] = df['Query'].apply(process_query)\n",
    "# df['processed_string'] = df['processed'].apply(lambda tokens: ' '.join(tokens))\n",
    "# df.to_csv('processed_queries.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
